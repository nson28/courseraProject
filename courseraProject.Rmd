---
title: "Practical Machine Learning Project"
author: "Nelson H. Tejara"
date: "23 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This document presents a way on how to build a predictive model for the Weight Lifting EXercises Dataset found [here]  (http://groupware.les.inf.puc-rio.br/har). The  goal of this project is to determine the manner in which the exercise is performed. The  random forest model was used by the author in order to develop the  predictive model.

# Data Acquisition and Cleaning

Firstly, the training and testing data has been downloaded. These data will be used for the training and validation of the  predictive model.
From the  data obtained some variables were of factor class. Upon checking, these variable it contains numeric entries that represents their levels.


Some preliminary observations includes :
* Some variables are useless (e.g. `levels(pml.data$kurtosis_yaw_belt)` -> "#DIV/0!")
* Some have actually a wide range of numeric values but also a "#DIV/0!" value.



```{r warning=FALSE, cache=TRUE}
pml.data <- read.csv("data/pml-training.csv", na.strings=c("NA",""))
# str(pml.data)
# check levels of factor variables to see those with useless levels and 
# see which variables should be numeric
# The code below is used to take a look at the  factor variables levels
#for (colName in colnames(pml.data[ ,sapply(pml.data, class) == "factor"])) {
#  print(colName)
#  print(levels(pml.data[[colName]]))
#}
# Remove useless variables from the data set.
variables <- c(
  "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
  "kurtosis_yaw_belt", "skewness_yaw_belt", "amplitude_yaw_belt", "cvtd_timestamp",
  "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "amplitude_yaw_dumbbell",
  "kurtosis_yaw_forearm", "skewness_yaw_forearm", "amplitude_yaw_forearm"
)
pml.data <- pml.data[ , -which(names(pml.data) %in% variables)]


# Convert factor variables which are actually numeric, to numeric variables.
variables <- c(
  "kurtosis_roll_belt", "kurtosis_picth_belt", "skewness_roll_belt",
  "skewness_roll_belt.1", "max_yaw_belt", "min_yaw_belt",
  "kurtosis_roll_arm","kurtosis_picth_arm", "kurtosis_yaw_arm",
  "skewness_roll_arm","skewness_pitch_arm", "skewness_yaw_arm",
  "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell","skewness_roll_dumbbell",
  "skewness_pitch_dumbbell","max_yaw_dumbbell", "min_yaw_dumbbell",
  "kurtosis_roll_forearm","kurtosis_picth_forearm", "skewness_roll_forearm",
  "skewness_pitch_forearm", "max_yaw_forearm", "min_yaw_forearm"
)
for (variable in variables) {
  pml.data[[variable]] <- as.numeric(as.character(pml.data[[variable]]))
}
```


It was found out that there are many variable that contains NA values. These values will affect during the training of our predictive model.To do this, a model will be build with no NA values at all utilizing only those variables with no NA values. If this will not work, a technique for gap filling will be employed in order to cope the NA values (eg. k- nearest neighbor). 

```{r cache=TRUE}
pml.data.naCounts <- colSums(sapply(pml.data, is.na))
pml.data.complete <- pml.data[,pml.data.naCounts == 0]
```

We have gone from `r ncol(pml.data)` variables to `r ncol(pml.data.complete)` variables.


# Training the model

The next step is to train the model. The  data is split to training and testing set to follow the  cross validation practices. As recommended from the  course, it is 60/40 for training and testing.


```{r cache=TRUE}
library(caret)
set.seed(20052015) # Set a seed for reproducibility purposes
# Split the training set in two parts: 60/40. The first part will be used for
# train the model, and the second part will be used for validation of the model.
inTrain <- createDataPartition(pml.data.complete$classe, p=0.6, list = FALSE)
pml.data.train <- pml.data.complete[inTrain,]
pml.data.test <- pml.data.complete[-inTrain,]
```

 

Before fitting the model, checking must be done to see if the  training contains numeric values that have no or close to zero variance.

```{r cache=TRUE}
nearZeroVar(pml.data.train[, sapply(pml.data.train, is.numeric)])
```


Since there are no numeric variables with near zero variance, one cannot reduce the  number of variables this way. Hence, we can continue building the  model utilizing all the  remaining variables in the  training dataset. 



```{r cache=TRUE}
library(caret)
modelFit <- train(classe ~ ., data = pml.data.train, method="rf", importance=TRUE)
```

# Verify the mode
```{r cache=TRUE}
# Now test the model with the test data.
predictionResults <- confusionMatrix(pml.data.test$classe, predict(modelFit, pml.data.test))
predictionResults$table
```

The total number of predictions is: `nrow(pml.data.test)`: `r nrow(pml.data.test)`.
This data is equal to the the sum of all items in the prediction matrix `sum(predictionResults$table)`: `r sum(predictionResults$table)`.
Only the diagonal represent counts of correct predicted results.
As the predicted results were generated with data which was not used for testing, we can calculate the out-of-sample error rate by substracting the sum of the diagonal from the toal: `sum(predictionResults$table) - sum(diag(predictionResults$table))`: `r sum(predictionResults$table) - sum(diag(predictionResults$table))` out of `r sum(predictionResults$table)` (or `r (sum(predictionResults$table) - sum(diag(predictionResults$table))) / sum(predictionResults$table) * 100`%).
In other words, the model succeeded quite well in predicting the activities on the training data set.



# Predict "new" data

Now we have a model that seems to perform well, we use it to predict the verification data.
Since we now have the  model that performs quiet well, then we can use it to predict the  verification data.

```{r cache=TRUE}
pml.verification <- read.csv("data/pml-testing.csv")
pml.verification <- pml.verification[, colnames(pml.verification) %in% colnames(pml.data.train)]
predict(modelFit, pml.verification)
```

# Appendix: 

The variables from `ncol pml.data.train` have been used. When we look at the variable importances in our model, we see a rather quick drop off.

```{r cache=TRUE}
variable.importances <- varImp(modelFit)
plot(variable.importances)
```

With this, one can see if he can train a model with lower number of variables having similar precision. To do this, one needs to get the row of sums of the  variable importances and normalize their values.


```{r cache=TRUE}
variable.importances.sums <- sort(rowSums(variable.importances$importance), decreasing = T)
variable.importances.sums <- variable.importances.sums / sum(variable.importances.sums)
```

If we take the first 16 variables we have captured about 50 percent of the overall importances:

```{r cache=TRUE}
sum(variable.importances.sums[1:16])
new.train.vars <- c("classe", names(variable.importances.sums[1:16]))
new.train.data <- pml.data.train[, colnames(pml.data.train) %in% new.train.vars]
new.test.data <- pml.data.test[, colnames(pml.data.train) %in% new.train.vars]
```

```{r cache=TRUE}
modelFit.reduced <- train(classe ~ ., data = new.train.data, method="rf", importance=TRUE)
```


Using the new mode1, it can be use again to test the data.

```{r cache=TRUE}
confusionMatrix(new.test.data$classe, predict(modelFit.reduced, new.test.data))
```
As reflected, it was able to produce as good results as the first. The training of this model took about 30 minutes as compared to ~1.5 hours for the first model.

